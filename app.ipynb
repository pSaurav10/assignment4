{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset from sklearn\n",
    "# The dataset contains features related to the characteristics of cell nuclei present in breast cancer biopsies\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a pandas DataFrame for easier manipulation\n",
    "# 'data' contains the features and 'target' contains the labels (0 = malignant, 1 = benign)\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: Select the top 10 features using SelectKBest\n",
    "# SelectKBest selects features that have the highest correlation with the target variable (y) using the ANOVA F-value test\n",
    "selector = SelectKBest(f_classif, k=10)  # Select the 10 most relevant features\n",
    "X_selected = selector.fit_transform(X, y)  # Fit the selector and apply it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['mean radius', 'mean perimeter', 'mean area', 'mean concavity',\n",
      "       'mean concave points', 'worst radius', 'worst perimeter', 'worst area',\n",
      "       'worst concavity', 'worst concave points'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selector.get_support()]  # Get the column names for selected features\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected feature names to a file for later use \n",
    "with open('selected_features.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets (using only the selected features)\n",
    "# This helps evaluate the model's performance on unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "# StandardScaler standardizes the data by removing the mean and scaling to unit variance\n",
    "# This ensures that all features are on the same scale, which helps many machine learning algorithms perform better\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform the test data (without fitting to prevent data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler to a file for later use in the Streamlit app\n",
    "# Save the scaler to a pickle file\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: MLPClassifier (Multi-layer Perceptron)\n",
    "# MLPClassifier is a neural network model that can be used for classification tasks\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)  # Specify a maximum of 500 iterations for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best parameters found: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (200,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Best cross-validation score: 0.9648\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for GridSearchCV\n",
    "# GridSearchCV will try all combinations of parameters to find the best model\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,)],  # Different sizes for the hidden layers\n",
    "    'activation': ['tanh', 'relu'],  # Activation functions (non-linearities)\n",
    "    'solver': ['adam', 'sgd'],  # Solvers for optimization (adam is faster and often more reliable)\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularization parameter to avoid overfitting\n",
    "    'learning_rate': ['constant', 'adaptive']  # How the learning rate changes during training\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters for the model\n",
    "# We use 5-fold cross-validation to evaluate each combination of parameters\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV on the training data to find the best model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Output the best hyperparameters and the corresponding score (accuracy) from GridSearchCV\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from GridSearchCV (the one with the best hyperparameters)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set using classification metrics\n",
    "# We predict the labels for the test set and compare them to the true labels\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, and selected features saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the best model to a file for use in a Streamlit app or for future use\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Print a confirmation message to indicate the model, scaler, and selected features have been saved successfully\n",
    "print(\"Model, scaler, and selected features saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
